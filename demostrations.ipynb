{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 236605 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tvtf\n",
    "from ECG_multi_lead_dataloader import *\n",
    "import transforms as tf\n",
    "import torchvision\n",
    "import torch\n",
    "import knn_classifier as knn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms that should be applied to each ECG record before returning it\n",
    "tf_ds = tvtf.Compose([\n",
    "    tf.ECG_tuple_transform(-1) # Reshape to 1D Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'/home/noamkeidar/vadimDB/'\n",
    "\n",
    "ECG_test = ECG_Multilead_Dataset(root_dir=root_dir, transform=tf_ds) # For KNN demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "num_train = 35000\n",
    "num_test = 1000\n",
    "batch_size = 10000\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ECG_test, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ECG_test, num_test, offset=num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)\n",
    "\n",
    "# Get all test data to predict in one go\n",
    "test_iter = iter(dl_test)\n",
    "x_test, y_test = test_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.70%\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = knn.KNNClassifier(k=10)\n",
    "knn_classifier.train(dl_train)\n",
    "y_pred = knn_classifier.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = knn.accuracy(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digitized 12 Lead ECG classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the environment from the previous exmple to free the memory (if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and preperations, change root_dir here to the directory of the data pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import models\n",
    "import transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from ECG_multi_lead_dataloader import *\n",
    "\n",
    "##### Change root direrctory here #####\n",
    "root_dir = r'/home/noamkeidar/vadimDB/'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database (might take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECG_Multilead_Dataset(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "\n",
    "# for real training:\n",
    "num_train = 35000\n",
    "# for small set overfit experiment: \n",
    "#num_train = 3500\n",
    "num_test = 1000\n",
    "batch_size = 512\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ds, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size= batch_size,shuffle=True)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ds, num_test, offset= num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did we load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long lead data of shape:  torch.Size([512, 1, 5000])\n",
      "Short lead data of shape:  torch.Size([512, 12, 1250])\n",
      "Labels of shape:  torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "x, y = iter(dl_train).next()\n",
    "x1, x2 = x\n",
    "\n",
    "print('Long lead data of shape: ', x2.shape)\n",
    "print('Short lead data of shape: ', x1.shape)\n",
    "print('Labels of shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this box to determine the archetecture of the digitized ECG classifing model. Note that the input data is tuples structured $(x_1,x_2)$, on which $x_1$ is a 12 on $N$ matrix containing digitized signals of short 12 lead ECG and $x_2$ is a 1 on $4\\cdot N$ matrix containing the long lead. Both inputs enter the model through 1d CNNs and combined to a single set of features before finally being cassified by a simple feedforward NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNNs structure: \n",
    "\n",
    "# num of channels and kernel length in each layer of each branch, note that list lengths must correspond \n",
    "short_hidden_channels = [16, 32, 64, 128, 256, 512]\n",
    "long_hidden_channels  = [4, 8, 16, 32, 64, 128, 256, 512]\n",
    "short_kernel_lengths = [5]*6\n",
    "long_kernel_lengths = [5]*8\n",
    "\n",
    "# which tricks to use: dropout, stride, batch normalization and dilation \n",
    "short_dropout = 0.5\n",
    "long_dropout = 0.5\n",
    "short_stride = 2\n",
    "long_stride = 2\n",
    "short_dilation = 1\n",
    "long_dilation = 1\n",
    "short_batch_norm = True\n",
    "long_batch_norm = True\n",
    "\n",
    "# enter input length here\n",
    "short_input_length = 1250\n",
    "long_input_length = 5000\n",
    "\n",
    "### FC net structure:\n",
    "\n",
    "# num of hidden units in every FC layer\n",
    "fc_hidden_dims = [128]\n",
    "\n",
    "# num of output classess \n",
    "num_of_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecg12LeadNet(\n",
      "  (short_cnn): ConvNet(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv1d(12, 16, kernel_size=(5,), stride=(2,))\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.5)\n",
      "      (3): ReLU()\n",
      "      (4): Conv1d(16, 32, kernel_size=(5,), stride=(2,))\n",
      "      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.5)\n",
      "      (7): ReLU()\n",
      "      (8): Conv1d(32, 64, kernel_size=(5,), stride=(2,))\n",
      "      (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.5)\n",
      "      (11): ReLU()\n",
      "      (12): Conv1d(64, 128, kernel_size=(5,), stride=(2,))\n",
      "      (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): Dropout(p=0.5)\n",
      "      (15): ReLU()\n",
      "      (16): Conv1d(128, 256, kernel_size=(5,), stride=(2,))\n",
      "      (17): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (18): Dropout(p=0.5)\n",
      "      (19): ReLU()\n",
      "      (20): Conv1d(256, 512, kernel_size=(5,), stride=(2,))\n",
      "      (21): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (22): Dropout(p=0.5)\n",
      "      (23): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (long_cnn): ConvNet(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv1d(1, 4, kernel_size=(5,), stride=(2,))\n",
      "      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.5)\n",
      "      (3): ReLU()\n",
      "      (4): Conv1d(4, 8, kernel_size=(5,), stride=(2,))\n",
      "      (5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.5)\n",
      "      (7): ReLU()\n",
      "      (8): Conv1d(8, 16, kernel_size=(5,), stride=(2,))\n",
      "      (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.5)\n",
      "      (11): ReLU()\n",
      "      (12): Conv1d(16, 32, kernel_size=(5,), stride=(2,))\n",
      "      (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): Dropout(p=0.5)\n",
      "      (15): ReLU()\n",
      "      (16): Conv1d(32, 64, kernel_size=(5,), stride=(2,))\n",
      "      (17): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (18): Dropout(p=0.5)\n",
      "      (19): ReLU()\n",
      "      (20): Conv1d(64, 128, kernel_size=(5,), stride=(2,))\n",
      "      (21): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (22): Dropout(p=0.5)\n",
      "      (23): ReLU()\n",
      "      (24): Conv1d(128, 256, kernel_size=(5,), stride=(2,))\n",
      "      (25): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (26): Dropout(p=0.5)\n",
      "      (27): ReLU()\n",
      "      (28): Conv1d(256, 512, kernel_size=(5,), stride=(2,))\n",
      "      (29): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (30): Dropout(p=0.5)\n",
      "      (31): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.Ecg12LeadNet(short_hidden_channels, long_hidden_channels,\n",
    "                 short_kernel_lengths, long_kernel_lengths,\n",
    "                 fc_hidden_dims,\n",
    "                 short_dropout, long_dropout,\n",
    "                 short_stride, long_stride,\n",
    "                 short_dilation, long_dilation,\n",
    "                 short_batch_norm, long_batch_norm,\n",
    "                 short_input_length, long_input_length,\n",
    "                 num_of_classes).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model on a single batch to make sure the dimentions fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output batch size is: 512 , and number of class scores: 1 \n",
      "\n",
      "26.953125 % Accurecy... maybe we should consider training the model\n"
     ]
    }
   ],
   "source": [
    "x_try = (x1.to(device, dtype=torch.float), x2.to(device, dtype=torch.float))\n",
    "y_pred = model(x_try)\n",
    "print('Output batch size is:', y_pred.shape[0], ', and number of class scores:', y_pred.shape[1],'\\n')\n",
    "\n",
    "num_correct = torch.sum((y_pred > 0).flatten() == (y.to(device, dtype=torch.long)==1))\n",
    "print(100*num_correct.item()/len(y), '% Accurecy... maybe we should consider training the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the game begin - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training section is based on the course abstruct Trainer class from HW 3. We have implemented a custom class for our model inhereting from Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/25 ---\n",
      "train_batch (Avg. Loss 0.035, Accuracy 98.9): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.092, Accuracy 96.4): 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "--- EPOCH 2/25 ---\n",
      "train_batch (Avg. Loss 0.086, Accuracy 97.1): 100%|██████████| 69/69 [00:15<00:00,  4.36it/s]\n",
      "test_batch (Avg. Loss 0.095, Accuracy 96.3): 100%|██████████| 2/2 [00:00<00:00,  8.75it/s]\n",
      "--- EPOCH 3/25 ---\n",
      "train_batch (Avg. Loss 0.070, Accuracy 97.5): 100%|██████████| 69/69 [00:15<00:00,  4.37it/s]\n",
      "test_batch (Avg. Loss 0.115, Accuracy 96.7): 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "*** Saved checkpoint checkpoints/Ecg12LeadNet.pt at epoch 3\n",
      "--- EPOCH 4/25 ---\n",
      "train_batch (Avg. Loss 0.066, Accuracy 97.7): 100%|██████████| 69/69 [00:15<00:00,  4.37it/s]\n",
      "test_batch (Avg. Loss 0.112, Accuracy 95.6): 100%|██████████| 2/2 [00:00<00:00,  8.71it/s]\n",
      "--- EPOCH 5/25 ---\n",
      "train_batch (Avg. Loss 0.062, Accuracy 97.8): 100%|██████████| 69/69 [00:15<00:00,  4.33it/s]\n",
      "test_batch (Avg. Loss 0.091, Accuracy 96.7): 100%|██████████| 2/2 [00:00<00:00,  7.76it/s]\n",
      "--- EPOCH 6/25 ---\n",
      "train_batch (Avg. Loss 0.061, Accuracy 97.8): 100%|██████████| 69/69 [00:15<00:00,  4.33it/s]\n",
      "test_batch (Avg. Loss 0.094, Accuracy 95.8): 100%|██████████| 2/2 [00:00<00:00,  8.81it/s]\n",
      "--- EPOCH 7/25 ---\n",
      "train_batch (Avg. Loss 0.062, Accuracy 97.7): 100%|██████████| 69/69 [00:15<00:00,  4.35it/s]\n",
      "test_batch (Avg. Loss 0.125, Accuracy 95.2): 100%|██████████| 2/2 [00:00<00:00,  8.74it/s]\n",
      "--- EPOCH 8/25 ---\n",
      "train_batch (Avg. Loss 0.057, Accuracy 97.9): 100%|██████████| 69/69 [00:15<00:00,  4.35it/s]\n",
      "test_batch (Avg. Loss 0.107, Accuracy 95.7): 100%|██████████| 2/2 [00:00<00:00,  8.70it/s]\n",
      "--- EPOCH 9/25 ---\n",
      "train_batch (Avg. Loss 0.060, Accuracy 97.9): 100%|██████████| 69/69 [00:15<00:00,  4.31it/s]\n",
      "test_batch (Avg. Loss 0.095, Accuracy 96.4): 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "--- EPOCH 10/25 ---\n",
      "train_batch (Avg. Loss 0.055, Accuracy 98.0): 100%|██████████| 69/69 [00:16<00:00,  4.31it/s]\n",
      "test_batch (Avg. Loss 0.111, Accuracy 95.6): 100%|██████████| 2/2 [00:00<00:00,  8.76it/s]\n",
      "--- EPOCH 11/25 ---\n",
      "train_batch (Avg. Loss 0.057, Accuracy 98.0): 100%|██████████| 69/69 [00:15<00:00,  4.36it/s]\n",
      "test_batch (Avg. Loss 0.090, Accuracy 96.4): 100%|██████████| 2/2 [00:00<00:00,  8.81it/s]\n",
      "--- EPOCH 12/25 ---\n",
      "train_batch (Avg. Loss 0.055, Accuracy 98.1): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.091, Accuracy 96.7): 100%|██████████| 2/2 [00:00<00:00,  8.31it/s]\n",
      "--- EPOCH 13/25 ---\n",
      "train_batch (Avg. Loss 0.054, Accuracy 98.0): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.079, Accuracy 97.4): 100%|██████████| 2/2 [00:00<00:00,  7.57it/s]\n",
      "*** Saved checkpoint checkpoints/Ecg12LeadNet.pt at epoch 13\n",
      "--- EPOCH 14/25 ---\n",
      "train_batch (Avg. Loss 0.051, Accuracy 98.2): 100%|██████████| 69/69 [00:15<00:00,  4.31it/s]\n",
      "test_batch (Avg. Loss 0.114, Accuracy 96.4): 100%|██████████| 2/2 [00:00<00:00,  8.70it/s]\n",
      "--- EPOCH 15/25 ---\n",
      "train_batch (Avg. Loss 0.052, Accuracy 98.1): 100%|██████████| 69/69 [00:15<00:00,  4.37it/s]\n",
      "test_batch (Avg. Loss 0.128, Accuracy 94.9): 100%|██████████| 2/2 [00:00<00:00,  8.80it/s]\n",
      "--- EPOCH 16/25 ---\n",
      "train_batch (Avg. Loss 0.048, Accuracy 98.3): 100%|██████████| 69/69 [00:15<00:00,  4.33it/s]\n",
      "test_batch (Avg. Loss 0.096, Accuracy 96.1): 100%|██████████| 2/2 [00:00<00:00,  8.29it/s]\n",
      "--- EPOCH 17/25 ---\n",
      "train_batch (Avg. Loss 0.051, Accuracy 98.2): 100%|██████████| 69/69 [00:16<00:00,  4.31it/s]\n",
      "test_batch (Avg. Loss 0.141, Accuracy 94.5): 100%|██████████| 2/2 [00:00<00:00,  8.18it/s]\n",
      "--- EPOCH 18/25 ---\n",
      "train_batch (Avg. Loss 0.047, Accuracy 98.3): 100%|██████████| 69/69 [00:16<00:00,  4.30it/s]\n",
      "test_batch (Avg. Loss 0.167, Accuracy 96.5): 100%|██████████| 2/2 [00:00<00:00,  8.75it/s]\n",
      "--- EPOCH 19/25 ---\n",
      "train_batch (Avg. Loss 0.047, Accuracy 98.4): 100%|██████████| 69/69 [00:15<00:00,  4.36it/s]\n",
      "test_batch (Avg. Loss 0.086, Accuracy 96.6): 100%|██████████| 2/2 [00:00<00:00,  8.76it/s]\n",
      "--- EPOCH 20/25 ---\n",
      "train_batch (Avg. Loss 0.049, Accuracy 98.2): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.123, Accuracy 94.7): 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "--- EPOCH 21/25 ---\n",
      "train_batch (Avg. Loss 0.048, Accuracy 98.3): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.124, Accuracy 94.9): 100%|██████████| 2/2 [00:00<00:00,  8.64it/s]\n",
      "--- EPOCH 22/25 ---\n",
      "train_batch (Avg. Loss 0.043, Accuracy 98.5): 100%|██████████| 69/69 [00:16<00:00,  4.31it/s]\n",
      "test_batch (Avg. Loss 0.103, Accuracy 95.2): 100%|██████████| 2/2 [00:00<00:00,  8.67it/s]\n",
      "--- EPOCH 23/25 ---\n",
      "train_batch (Avg. Loss 0.045, Accuracy 98.4): 100%|██████████| 69/69 [00:15<00:00,  4.35it/s]\n",
      "test_batch (Avg. Loss 0.111, Accuracy 96.0): 100%|██████████| 2/2 [00:00<00:00,  8.77it/s]\n",
      "--- EPOCH 24/25 ---\n",
      "train_batch (Avg. Loss 0.042, Accuracy 98.5): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.117, Accuracy 96.7): 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "--- EPOCH 25/25 ---\n",
      "train_batch (Avg. Loss 0.043, Accuracy 98.5): 100%|██████████| 69/69 [00:15<00:00,  4.32it/s]\n",
      "test_batch (Avg. Loss 0.081, Accuracy 97.5): 100%|██████████| 2/2 [00:00<00:00,  8.63it/s]\n",
      "*** Saved checkpoint checkpoints/Ecg12LeadNet.pt at epoch 25\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from training import Ecg12LeadNetTrainerBinary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 25\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = Ecg12LeadNetTrainerBinary(model, loss_fn, optimizer, device)\n",
    "\n",
    "fitResult = trainer.fit(dl_train, dl_test, num_epochs, checkpoints=r'checkpoints/Ecg12LeadNet',\n",
    "                        early_stopping=10, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_batch (Avg. Loss 0.081, Accuracy 97.5): 100%|██████████| 2/2 [00:00<00:00,  6.21it/s]\n",
      "Test accurecy is:  97.5 %\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test_epoch(dl_test, verbose=True)\n",
    "print('Test accurecy is: ', test_result[1], '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
