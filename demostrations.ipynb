{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 236605 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tvtf\n",
    "from ECG_multi_lead_dataloader import *\n",
    "import transforms as tf\n",
    "import torchvision\n",
    "import torch\n",
    "import knn_classifier as knn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms that should be applied to each ECG record before returning it\n",
    "tf_ds = tvtf.Compose([\n",
    "    tf.ECG_tuple_transform(-1) # Reshape to 1D Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\noam\\Desktop\\vadimDB'+'\\\\'\n",
    "\n",
    "ECG_test=ECG_Multilead_Dataset(root_dir=root_dir,transform= tf_ds) # For KNN demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "num_train = 35000\n",
    "num_test = 1000\n",
    "batch_size = 10000\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ECG_test, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size= batch_size,shuffle=False)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ECG_test, num_test, offset= num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)\n",
    "\n",
    "# Get all test data to predict in one go\n",
    "test_iter = iter(dl_test)\n",
    "x_test, y_test = test_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.50%\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = knn.KNNClassifier(k=10)\n",
    "knn_classifier.train(dl_train)\n",
    "y_pred = knn_classifier.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = knn.accuracy(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digitized 12 Lead ECG classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the environment from the previous exmple to free the memory (if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and preperations, change root_dir here to the directory of the data pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import models\n",
    "import transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from ECG_multi_lead_dataloader import *\n",
    "\n",
    "##### Change root direrctory here #####\n",
    "root_dir = r'C:\\Users\\noam\\Desktop\\vadimDB'+'\\\\'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database (might take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECG_Multilead_Dataset(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "\n",
    "# for real training:\n",
    "#num_train = 35000\n",
    "# for small set overfit experiment: \n",
    "num_train = 3500\n",
    "num_test = 1000\n",
    "batch_size = 128\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ds, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size= batch_size,shuffle=True)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ds, num_test, offset= num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did we load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long lead data of shape:  torch.Size([128, 1, 5000])\n",
      "Short lead data of shape:  torch.Size([128, 12, 1250])\n",
      "Labels of shape:  torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "x, y = iter(dl_train).next()\n",
    "x1, x2 = x\n",
    "\n",
    "print('Long lead data of shape: ', x2.shape)\n",
    "print('Short lead data of shape: ', x1.shape)\n",
    "print('Labels of shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this box to determine the archetecture of the digitized ECG classifing model. Note that the input data is tuples structured $(x_1,x_2)$, on which $x_1$ is a 12 on $N$ matrix containing digitized signals of short 12 lead ECG and $x_2$ is a 1 on $4\\cdot N$ matrix containing the long lead. Both inputs enter the model through 1d CNNs and combined to a single set of features before finally being cassified by a simple feedforward NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNNs structure: \n",
    "\n",
    "# num of channels and kernel length in each layer of each branch, note that list lengths must correspond \n",
    "short_hidden_channels = [16, 32, 64, 128, 256, 512]\n",
    "long_hidden_channels  = [4, 8, 16, 32, 64, 128, 256, 512]\n",
    "short_kernel_lengths = [5]*6\n",
    "long_kernel_lengths = [5]*8\n",
    "\n",
    "# which tricks to use: dropout, stride, batch normalization and dilation \n",
    "short_dropout = None\n",
    "long_dropout = None\n",
    "short_stride = 2\n",
    "long_stride = 2\n",
    "short_dilation = 1\n",
    "long_dilation = 1\n",
    "short_batch_norm = True\n",
    "long_batch_norm = True\n",
    "\n",
    "# enter input length here\n",
    "short_input_length = 1250\n",
    "long_input_length = 5000\n",
    "\n",
    "### FC net structure:\n",
    "\n",
    "# num of hidden units in every FC layer\n",
    "fc_hidden_dims = [128]\n",
    "\n",
    "# num of output classess \n",
    "num_of_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecg12LeadNet(\n",
      "  (short_cnn): ConvNet(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv1d(12, 16, kernel_size=(5,), stride=(2,))\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv1d(16, 32, kernel_size=(5,), stride=(2,))\n",
      "      (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "      (6): Conv1d(32, 64, kernel_size=(5,), stride=(2,))\n",
      "      (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "      (9): Conv1d(64, 128, kernel_size=(5,), stride=(2,))\n",
      "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ReLU()\n",
      "      (12): Conv1d(128, 256, kernel_size=(5,), stride=(2,))\n",
      "      (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU()\n",
      "      (15): Conv1d(256, 512, kernel_size=(5,), stride=(2,))\n",
      "      (16): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (long_cnn): ConvNet(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv1d(1, 4, kernel_size=(5,), stride=(2,))\n",
      "      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv1d(4, 8, kernel_size=(5,), stride=(2,))\n",
      "      (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "      (6): Conv1d(8, 16, kernel_size=(5,), stride=(2,))\n",
      "      (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "      (9): Conv1d(16, 32, kernel_size=(5,), stride=(2,))\n",
      "      (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ReLU()\n",
      "      (12): Conv1d(32, 64, kernel_size=(5,), stride=(2,))\n",
      "      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU()\n",
      "      (15): Conv1d(64, 128, kernel_size=(5,), stride=(2,))\n",
      "      (16): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): ReLU()\n",
      "      (18): Conv1d(128, 256, kernel_size=(5,), stride=(2,))\n",
      "      (19): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (20): ReLU()\n",
      "      (21): Conv1d(256, 512, kernel_size=(5,), stride=(2,))\n",
      "      (22): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (23): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "\n",
    "model = models.Ecg12LeadNet(short_hidden_channels, long_hidden_channels,\n",
    "                 short_kernel_lengths, long_kernel_lengths,\n",
    "                 fc_hidden_dims,\n",
    "                 short_dropout, long_dropout,\n",
    "                 short_stride, long_stride,\n",
    "                 short_dilation, long_dilation,\n",
    "                 short_batch_norm, long_batch_norm,\n",
    "                 short_input_length, long_input_length,\n",
    "                 num_of_classes).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model on a single batch to make sure the dimentions fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output batch size is: 128 , and number of class scores: 1 \n",
      "\n",
      "47.65625 % Accurecy... maybe we should consider training the model\n"
     ]
    }
   ],
   "source": [
    "x_try = (x1.to(device, dtype=torch.float), x2.to(device, dtype=torch.float))\n",
    "y_pred = model(x_try)\n",
    "print('Output batch size is:', y_pred.shape[0], ', and number of class scores:', y_pred.shape[1],'\\n')\n",
    "\n",
    "num_correct = torch.sum((y_pred > 0).flatten() == (y.to(device, dtype=torch.long)==1))\n",
    "print(100*num_correct.item()/len(y), '% Accurecy... maybe we should consider training the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the game begin - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training section is based on the course abstruct Trainer class from HW 3. We have implemented a custom class for our model inhereting from Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch (Avg. Loss 0.000, Accuracy 100.0): 100%|███████████████████████████████████| 28/28 [00:08<00:00,  3.34it/s]\n",
      "\n",
      "Epoch #1: Avg. loss = 0.000, Accuracy = 100.00%\n",
      "train_batch (Avg. Loss 0.000, Accuracy 100.0): 100%|███████████████████████████████████| 28/28 [00:09<00:00,  3.14it/s]\n",
      "\n",
      "Epoch #2: Avg. loss = 0.000, Accuracy = 100.00%\n",
      "train_batch (Avg. Loss 0.000, Accuracy 100.0): 100%|███████████████████████████████████| 28/28 [00:10<00:00,  2.68it/s]\n",
      "\n",
      "Epoch #3: Avg. loss = 0.000, Accuracy = 100.00%\n",
      "train_batch (Avg. Loss 0.000, Accuracy 100.0): 100%|███████████████████████████████████| 28/28 [00:11<00:00,  2.59it/s]\n",
      "\n",
      "Epoch #4: Avg. loss = 0.000, Accuracy = 100.00%\n",
      "train_batch (Avg. Loss 0.000, Accuracy 100.0): 100%|███████████████████████████████████| 28/28 [00:10<00:00,  3.23it/s]\n",
      "\n",
      "Epoch #5: Avg. loss = 0.000, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from training import Ecg12LeadNetTrainerBinary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = Ecg12LeadNetTrainerBinary(model, loss_fn, optimizer, device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_result = trainer.train_epoch(dl_train, verbose=True)\n",
    "    \n",
    "    if epoch == 0 or (epoch+1) % 1 == 0:\n",
    "        avg_loss = np.mean(epoch_result.losses)\n",
    "        accuracy = np.mean(epoch_result.accuracy)\n",
    "        print(f'\\nEpoch #{epoch+1}: Avg. loss = {avg_loss:.3f}, Accuracy = {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_batch (Avg. Loss 1.055, Accuracy 89.9): 100%|███████████████████████████████████████| 8/8 [00:01<00:00,  7.45it/s]\n",
      "test accurecy is:  89.9 %\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test_epoch(dl_test, verbose=True)\n",
    "print('Test accurecy is: ', test_result[1], '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
