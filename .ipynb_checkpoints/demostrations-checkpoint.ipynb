{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cs236605 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By:\n",
    "Vadim Grainer & Noam Keidar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tvtf\n",
    "from ECG_multi_lead_dataloader import *\n",
    "import transforms as tf\n",
    "import torchvision\n",
    "import torch\n",
    "import knn_classifier as knn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms that should be applied to each ECG record before returning it\n",
    "tf_ds = tvtf.Compose([\n",
    "    tf.ECG_tuple_transform(-1) # Reshape to 1D Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading new format\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'C:\\Users\\noam\\Desktop\\vadimDBUnified\\short_leads_digitized0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ad7343683c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'C:\\Users\\noam\\Desktop\\vadimDBUnified'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mECG_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECG_Multilead_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_ds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For KNN demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ML/proj/2366605-Final-Project/ECG_multi_lead_dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, transform, partial_upload, new_format)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mclassification_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcntr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'short_leads_digitized'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcntr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mf_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs236605-hw/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs236605-hw/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid mode; must be one of r, r+, w, w-, x, a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muserblock_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'C:\\Users\\noam\\Desktop\\vadimDBUnified\\short_leads_digitized0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\noam\\Desktop\\vadimDBUnified'+'\\\\'\n",
    "\n",
    "ECG_test = ECG_Multilead_Dataset(root_dir=root_dir, transform=tf_ds) # For KNN demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "num_train = 35000\n",
    "num_test = 1000\n",
    "batch_size = 10000\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ECG_test, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ECG_test, num_test, offset=num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)\n",
    "\n",
    "# Get all test data to predict in one go\n",
    "test_iter = iter(dl_test)\n",
    "x_test, y_test = test_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = knn.KNNClassifier(k=10)\n",
    "knn_classifier.train(dl_train)\n",
    "y_pred = knn_classifier.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = knn.accuracy(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digitized 12 Lead ECG classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is intended to be a banchmark for the prefoarmance of the two ECG image classification paradigms. We assume that our knowladge that the real information in the ECG images is contained only in the signal and not the background will allow easier convergence of the model and better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the environment from the previous exmple to free the memory (if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and preperations, change root_dir here to the directory of the data pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import models\n",
    "import transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from ECG_multi_lead_dataloader import *\n",
    "\n",
    "##### Change root direrctory here #####\n",
    "root_dir = r'C:\\Users\\noam\\Desktop\\vadimDBUnified'+'\\\\'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database (might take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECG_Multilead_Dataset(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "\n",
    "# for real training:\n",
    "num_train = 35000\n",
    "# for small set overfit experiment: \n",
    "#num_train = 3500\n",
    "num_test = 1000\n",
    "batch_size = 512\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ds, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size= batch_size,shuffle=True)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ds, num_test, offset= num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did we load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = iter(dl_train).next()\n",
    "x1, x2 = x\n",
    "\n",
    "print('Long lead data of shape: ', x2.shape)\n",
    "print('Short lead data of shape: ', x1.shape)\n",
    "print('Labels of shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this box to determine the archetecture of the digitized ECG classifing model. Note that the input data is tuples structured $(x_1,x_2)$, on which $x_1$ is a 12 on $N$ matrix containing digitized signals of short 12 lead ECG and $x_2$ is a 1 on $4\\cdot N$ matrix containing the long lead. Both inputs enter the model through 1d CNNs and combined to a single set of features before finally being cassified by a simple feedforward NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNNs structure: \n",
    "\n",
    "# num of channels and kernel length in each layer of each branch, note that list lengths must correspond \n",
    "short_hidden_channels = [16, 32, 64, 128, 256, 512]\n",
    "long_hidden_channels  = [4, 8, 16, 32, 64, 128, 256, 512]\n",
    "short_kernel_lengths = [5]*6\n",
    "long_kernel_lengths = [5]*8\n",
    "\n",
    "# which tricks to use: dropout, stride, batch normalization and dilation \n",
    "short_dropout = 0.5\n",
    "long_dropout = 0.5\n",
    "short_stride = 2\n",
    "long_stride = 2\n",
    "short_dilation = 1\n",
    "long_dilation = 1\n",
    "short_batch_norm = True\n",
    "long_batch_norm = True\n",
    "\n",
    "# enter input length here\n",
    "short_input_length = 1250\n",
    "long_input_length = 5000\n",
    "\n",
    "### FC net structure:\n",
    "\n",
    "# num of hidden units in every FC layer\n",
    "fc_hidden_dims = [128]\n",
    "\n",
    "# num of output classess \n",
    "num_of_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Ecg12LeadNet(short_hidden_channels, long_hidden_channels,\n",
    "                 short_kernel_lengths, long_kernel_lengths,\n",
    "                 fc_hidden_dims,\n",
    "                 short_dropout, long_dropout,\n",
    "                 short_stride, long_stride,\n",
    "                 short_dilation, long_dilation,\n",
    "                 short_batch_norm, long_batch_norm,\n",
    "                 short_input_length, long_input_length,\n",
    "                 num_of_classes).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model on a single batch to make sure the dimentions fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_try = (x1.to(device, dtype=torch.float), x2.to(device, dtype=torch.float))\n",
    "y_pred = model(x_try)\n",
    "print('Output batch size is:', y_pred.shape[0], ', and number of class scores:', y_pred.shape[1],'\\n')\n",
    "\n",
    "num_correct = torch.sum((y_pred > 0).flatten() == (y.to(device, dtype=torch.long)==1))\n",
    "print(100*num_correct.item()/len(y), '% Accurecy... maybe we should consider training the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the game begin - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training section is based on the course abstruct Trainer class from HW 3. We have implemented a custom class for our model inhereting from Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from training import Ecg12LeadNetTrainerBinary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 25\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = Ecg12LeadNetTrainerBinary(model, loss_fn, optimizer, device)\n",
    "\n",
    "fitResult = trainer.fit(dl_train, dl_test, num_epochs, checkpoints=r'checkpoints/Ecg12LeadNet',\n",
    "                        early_stopping=10, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = trainer.test_epoch(dl_test, verbose=True)\n",
    "print('Test accurecy is: ', test_result[1], '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some intro... ############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "from ECG_rendered_multilead_dataloader import *\n",
    "import transforms as tf\n",
    "import torch\n",
    "import knn_classifier as knn\n",
    "\n",
    "# Define the transforms that should be applied to each ECG record before returning it\n",
    "tf_ds = tvtf.Compose([\n",
    "    tf.ECG_rendering_transform(-1) # Reshape to 1D Tensor\n",
    "])\n",
    "\n",
    "root_dir = r'C:\\Users\\noam\\Desktop\\vadimDBUnified'+'\\\\'\n",
    "ECG_test = ECG_Rendered_Multilead_Dataset(root_dir=root_dir, transform=tf_ds, partial_upload=False)\n",
    "K = ECG_test[10]\n",
    "\n",
    "print('Managed to upload the sample # 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 400\n",
    "num_test = 100\n",
    "batch_size = 100\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ECG_test, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size= batch_size,shuffle=True)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ECG_test, num_test, offset=num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test data to predict in one go\n",
    "test_iter = iter(dl_test)\n",
    "x_test, y_test = test_iter.next()\n",
    "\n",
    "train_iter = iter(dl_train)\n",
    "x_train, y_train = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test kNN Classifier\n",
    "knn_classifier = knn.KNNClassifier(k=10)\n",
    "knn_classifier.train(dl_train)\n",
    "y_pred = knn_classifier.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = knn.accuracy(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print('However, note that due to small test set, this result might be attributed to data imblance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('let us see how the train set tagging looks like:')\n",
    "print(y_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and preperations, change root_dir here to the directory of the data pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import models\n",
    "import transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from ECG_rendered_multilead_dataloader import *\n",
    "\n",
    "##### Change root direrctory here #####\n",
    "root_dir = r'/home/noamkeidar/ML/db/'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database (might take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECG_Rendered_Multilead_Dataset(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how much data to load (only use a subset for speed)\n",
    "\n",
    "# for real training:\n",
    "num_train = 35000\n",
    "# for small set overfit experiment: \n",
    "#num_train = 1025\n",
    "num_val = 1000\n",
    "num_test = 5000\n",
    "batch_size = 42\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ds, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Validation dataset & loader\n",
    "ds_val = tf.SubsetDataset(ds, num_val, offset=num_train)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ds, num_test, offset=num_train + num_val)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did we load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images of shape:  torch.Size([42, 675, 1450, 3])\n",
      "Labels of shape:  torch.Size([42])\n",
      "Size of a batch in the memory is: ~ 117 MB\n"
     ]
    }
   ],
   "source": [
    "x, y = iter(dl_train).next()\n",
    "\n",
    "in_h = x.shape[1]\n",
    "in_w = x.shape[2]\n",
    "in_channels = x.shape[3]\n",
    "batch_memory = x.element_size() * x.nelement() // 1024**2\n",
    "\n",
    "print('Images of shape: ', x.shape)\n",
    "print('Labels of shape: ', y.shape)\n",
    "print('Size of a batch in the memory is: ~', batch_memory,'MB' )\n",
    "\n",
    "x = x.transpose(1,2).transpose(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this box to determine the archetecture of the reandered ECG classifing model. Note that the input data is comprezed of big RGB images, so memory is a major consideration. The input image is fed into a 2d CNN to yield a set of features before finally being cassified by a simple feedforward NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNNs structure: \n",
    "\n",
    "# num of channels and kernel length in each layer, note that list lengths must correspond \n",
    "hidden_channels = [8, 16, 32, 64, 128, 256, 512]\n",
    "kernel_sizes = [5] * 7\n",
    "\n",
    "# which tricks to use: dropout, stride, batch normalization and dilation \n",
    "dropout = 0.5\n",
    "stride = 2\n",
    "dilation = 1\n",
    "batch_norm = True\n",
    "\n",
    "### FC net structure:\n",
    "\n",
    "# num of hidden units in every FC layer\n",
    "fc_hidden_dims = [128]\n",
    "\n",
    "# num of output classess \n",
    "num_of_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecg12ImageNet(\n",
      "  (cnn): ConvNet2d(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 8, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.5)\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.5)\n",
      "      (7): ReLU()\n",
      "      (8): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.5)\n",
      "      (11): ReLU()\n",
      "      (12): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): Dropout(p=0.5)\n",
      "      (15): ReLU()\n",
      "      (16): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (18): Dropout(p=0.5)\n",
      "      (19): ReLU()\n",
      "      (20): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (22): Dropout(p=0.5)\n",
      "      (23): ReLU()\n",
      "      (24): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (26): Dropout(p=0.5)\n",
      "      (27): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.Ecg12ImageNet(in_channels, hidden_channels, kernel_sizes, in_h, in_w, \n",
    "                             fc_hidden_dims, dropout=dropout, stride=stride,\n",
    "                             dilation=dilation, batch_norm=batch_norm, num_of_classes=2).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model on a single batch to make sure the dimentions fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output batch size is: 42 , and number of class scores: 1 \n",
      "\n",
      "52.38095238095238 % Accurecy... maybe we should consider training the model\n"
     ]
    }
   ],
   "source": [
    "x_try = x.to(device, dtype=torch.float)\n",
    "y_pred = model(x_try)\n",
    "print('Output batch size is:', y_pred.shape[0], ', and number of class scores:', y_pred.shape[1],'\\n')\n",
    "\n",
    "num_correct = torch.sum((y_pred > 0).flatten() == (y.to(device, dtype=torch.long)==1))\n",
    "print(100*num_correct.item()/len(y), '% Accurecy... maybe we should consider training the model')\n",
    "\n",
    "del x, y, x_try, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the game begin - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training section is based on the course abstruct Trainer class from HW 3. We have implemented a custom class for our model inhereting from Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/15 ---\n",
      "train_batch (0.411):   2%|▏         | 14/834 [00:29<27:27,  2.01s/it]"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from training import Ecg12LeadImageNetTrainerBinary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 15\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = Ecg12LeadImageNetTrainerBinary(model, loss_fn, optimizer, device)\n",
    "\n",
    "fitResult = trainer.fit(dl_train, dl_val, num_epochs, checkpoints=r'checkpoints/Ecg12LeadImageNetWithDropout',\n",
    "                        early_stopping=5, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_batch (Avg. Loss 0.370, Accuracy 96.5): 100%|██████████| 120/120 [03:10<00:00,  1.59s/it]\n",
      "Test accurecy is:  96.52 %\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test_epoch(dl_test, verbose=True)\n",
    "print('Test accurecy is: ', test_result[1], '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to digitized signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we train models to extruct the digitized signal from a rendered image containing it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import Ecg12ImageToSignalNet\n",
    "import transforms as tf\n",
    "from ECG_rendered_to_matrix_DB_dataloader import *\n",
    "\n",
    "root_dir = r'C:\\Users\\noam\\Desktop\\vadimDBUnified'+'\\\\'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECG_Rendered_to_matrix_Dataset(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 2\n",
    "num_test = 1\n",
    "batch_size = 1\n",
    "\n",
    "# Training dataset & loader\n",
    "ds_train = tf.SubsetDataset(ds, num_train)  #(train=True, transform=tf_ds)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size= batch_size,shuffle=True)\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = tf.SubsetDataset(ds, num_test, offset= num_train)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "x, y = iter(dl_train).next()\n",
    "end = time.time()\n",
    "print(f'Time taken {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_memory = x.element_size() * x.nelement() // 1024**2\n",
    "\n",
    "print('Images of shape: ', x.shape)\n",
    "print('Short leads of shape: ', y[0].size())\n",
    "print('Long leads of shape: ', y[1].size())\n",
    "print('Size of an input batch in the memory is: ~', batch_memory,'MB' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_h = x.shape[1]\n",
    "in_w = x.shape[2]\n",
    "in_channels = x.shape[3]\n",
    "conv_hidden_channels = [8, 16, 32, 64, 128, 256, 512]\n",
    "conv_kernel_sizes = [5]*7\n",
    "\n",
    "deconv_in_channels = 1024\n",
    "deconv_hidden_channels_short = [512, 256, 128, 64, 32, 16]\n",
    "deconv_kernel_lengths_short = [5]*6\n",
    "deconv_hidden_channels_long = [512, 256, 128, 64, 32, 16, 8, 4]\n",
    "deconv_kernel_lengths_long = [5]*8\n",
    "\n",
    "conv_dropout = None\n",
    "conv_stride = 2\n",
    "conv_dilation = 1\n",
    "conv_batch_norm = True\n",
    "\n",
    "deconv_dropout_short = None\n",
    "deconv_stride_short = 2\n",
    "deconv_dilation_short = 1\n",
    "deconv_batch_norm_short = True\n",
    "deconv_out_kernel_short = 5\n",
    "deconv_dropout_long = None\n",
    "deconv_stride_long = 2\n",
    "deconv_dilation_long = 1\n",
    "deconv_batch_norm_long = True\n",
    "deconv_out_kernel_long = 5\n",
    "\n",
    "fc_hidden_dims=()\n",
    "\n",
    "l_out_long = 5000\n",
    "l_out_short = 1250\n",
    "short_leads = 12\n",
    "long_leads = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ecg12ImageToSignalNet(in_channels, deconv_in_channels, in_h, in_w, conv_hidden_channels, conv_kernel_sizes,\n",
    "                 deconv_hidden_channels_short, deconv_kernel_lengths_short,\n",
    "                 deconv_hidden_channels_long, deconv_kernel_lengths_long,\n",
    "                 conv_dropout, conv_stride, conv_dilation, conv_batch_norm,\n",
    "                 deconv_dropout_short, deconv_stride_short, deconv_dilation_short,\n",
    "                 deconv_batch_norm_short, deconv_out_kernel_short, deconv_dropout_long,\n",
    "                 deconv_stride_long, deconv_dilation_long,\n",
    "                 deconv_batch_norm_long, deconv_out_kernel_long, \n",
    "                 fc_hidden_dims, l_out_long, l_out_short, short_leads, long_leads).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_try = x.transpose(1,2).transpose(1,3)\n",
    "x_try = x_try.to(device, dtype=torch.float)\n",
    "y_pred = model(x_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0].shape)\n",
    "print(y_pred[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "t = np.arange(0, 10, 1/500)\n",
    "plt.plot(t, y_pred[1].flatten().tolist(), t, y[1].flatten().tolist())\n",
    "\n",
    "print('Maybe some training will make the reconstruction better...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the game begin - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from training import EcgImageToDigitizedTrainer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = EcgImageToDigitizedTrainer(model, loss_fn, optimizer, device)\n",
    "\n",
    "fitResult = trainer.fit(dl_train, dl_test, num_epochs, checkpoints=r'checkpoints/Ecg12LeadImageNet',\n",
    "                        early_stopping=100, print_every=10)\n",
    "\n",
    "x, y = iter(dl_train).next()\n",
    "x_try = x.transpose(1,2).transpose(1,3)\n",
    "x_try = x_try.to(device, dtype=torch.float)\n",
    "y_pred = model(x_try)\n",
    "t = np.arange(0, 10, 1/500)\n",
    "plt.plot(t, y_pred[1].flatten().tolist(), t, y[1].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dl_train)\n",
    "x, y = it.next()\n",
    "x_try = x.transpose(1,2).transpose(1,3)\n",
    "x_try = x_try.to(device, dtype=torch.float)\n",
    "y_pred = model(x_try)\n",
    "t = np.arange(0, 10, 1/500)\n",
    "plt.plot(t, y_pred[1].flatten().tolist(), t, y[1].flatten().tolist())\n",
    "plt.figure()\n",
    "x, y = it.next()\n",
    "x_try = x.transpose(1,2).transpose(1,3)\n",
    "x_try = x_try.to(device, dtype=torch.float)\n",
    "y_pred = model(x_try)\n",
    "t = np.arange(0, 10, 1/500)\n",
    "plt.plot(t, y_pred[1].flatten().tolist(), t, y[1].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
